<html>

<head>
  <title>Performance and Interpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models Workshop</title>
  <meta http-equiv="Pragma" content="no-cache" />
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300&display=swap" rel="stylesheet">

<style>

/* Style the tab */
.tab {
  overflow: hidden;
  border: 0px solid #ccc;
}

/* Style the buttons inside the tab */
.tab button {
  background-color: #cec1f0;
  float: left;
  border: 0.1rem solid #fff;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  transition: 0.3s;
  font-size: 17px;
  border-radius: 0%;
  color:#fff;
}

/* Change background color of buttons on hover */
.tab button:hover {
  background-color: #5a4fcf;
    top: -0.5rem;
    transition: top 0.25s;
    color:#fff;
}

/* Create an active/current tablink class */
.tab button.active {
  background-color: #5a4fcf;
  border-bottom: 0px solid #cec1f0;
}



.tab-label {
  position: relative;
  display: block;
  line-height: 2.75em;
  height: 3em;
  padding: 0 1.618em;
  background: #5a4fcf;
  border-right: 1.05rem solid #fff;
  color: #fff;
  cursor: pointer;
  top: 0;
  transition: all 0.25s;
}
.tab-label:hover {
}

/* Style the tab content */
.tabcontent {
  display: none;
  padding: 6px 12px;
  border: 0px solid #ccc;
  border-top: none;
}
</style>

</head>

<body height='100%' style="background-color: white;">

  <div style="float:left; margin:0rem 0rem 0rem 0.5rem;">
    <h3> <i class="fa fa-home" aria-hidden="true" style="color:white;"></i> </h3>
  </div>

  <div style="float:left; background-color: #5a4fcf;">
    <div class="center" style="float:left; margin:2rem 8rem 0rem 8rem;">
      <h1> Performance and Interpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models</h1>
    </div>

    <div class="center" style="float:left; margin:0rem 8rem 2rem 8rem;">
      <p> Workshop co-located virtually at <a>COLING 2022</a> (October 12-17, 2022)

        <br><br>
        Workshop Email: <b>mmmpie.workshop@gmail.com<b>

      </p>
    </div>


  </div>



  <div style="float:left;">


    <div style="float:left; margin:1rem 8rem -2.5rem 8rem; width:auto; ">
      <div class="tab" style="width:100%;">
        <button class="tablinks active" onclick="openTab(event, 'overview')">Overview</button>
        <button class="tablinks " onclick="openTab(event, 'cfp')">Call For Papers</button>
      </div>
    </div>

    <div style="float:left; margin:2rem 8rem 0rem 8rem;">
      <div id="overview" class="tabcontent" style="display:block;">


        <h3> Overview </h3>
        <p>

          With the increasing use of machine-learning driven algorithmic judgements
          to support or automate real-world systems and decision making, it is
          critical to understand the strengths and weaknesses beyond traditional
          aggregate performance evaluations. This is particularly critical for
          massive-scale, multimodal, or multipurpose (adapted or adaptable to
          multiple downstream tasks) models that are rapidly becoming the
          foundation for many applications in computational linguistics
          (e.g., <a href="https://aclanthology.org/N19-1423/" target="_blank">BERT</a>,
          <a href="https://aclanthology.org/D19-5602/" target="_blank">GPT2</a>,
          <a href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" target="_blank">GPT3</a>,
          <a href="http://proceedings.mlr.press/v139/radford21a/radford21a.pdf" target="_blank">CLIP</a>).
          Especially as these
          large, pretrained models are being adapted for use or the foundation of
          models in a wide-ranging variety of applications, we need evaluations
          beyond aggregate performance for informed use and to identify and mitigate biases.

          <br><br>

          We need to understand not only how a model performs overall but also
          why or under what circumstances the model will perform unreliably, and
          how interpretable the model and model outputs are in context of its use.
          Additionally, as new computational paradigms such as quantum computing
          are introduced, what new insights, challenges, and capabilities do models
          using these paradigms bring to model evaluations? Can we leverage these
          advances for more comprehensive evaluations?

          <br> <br>

          This workshop will focus on:
        <ol>
          <li>
            Evaluations that encompass multiple principles of responsible AI/ML
            (e.g., robustness, accountability, fairness, transparency, interpretability)
            for multimodal, multipurpose, and massive-scale models; and
          </li>
          <li>
            Research that examine the differences in evaluations for narrow (single modality,
            single task) and multipurpose or multimodal models, and nontraditional
            methods, measures, or metrics that can be leveraged.
          </li>
        </ol>

        <p>
          The workshop will include keynote talks, a panel discussion on open
          challenges and pathways forward for trusted and responsible evaluations
          of massive-scale, multimodal, or multipurpose and, a series of contributed
          talks highlighting paper submissions and a poster session to engage authors
          and attendees in more detailed discussions.

        </p>

        <br>
        <h3>Organizers</h3>

        <div style="float:left; margin:0rem 0rem 1rem 0rem;">
          <p>
            <a style="font-weight:bold; color:#5a4fcf;">
              Maria Glenski</a>, Pacific Northwest National Laboratory

            <br>
            <a style="font-weight:bold; color:#5a4fcf;">
              Yulia Tsvetkov</a>, University of Washington

            <br>
            <a style="font-weight:bold; color:#5a4fcf;">
              Megan Kohagen</a>, Quantinuum

            <br>
            <a style="font-weight:bold; color:#5a4fcf;">
              Vidhisha Balachandran</a>, Carnegie Mellon University


            <br><br><br>
            Workshop Email: <a style="color:#5a4fcf;">mmmpie.workshop@gmail.com</a>
          </p>
        </div>

      </div>

      <div id="cfp" class="tabcontent">


        <h3> Call for Papers </h3>
        <p>
          We welcome submissions that focus on performance and interpretability analyses
          or evaluation methods for multimodal, multipurpose, or massive-scale models.
          Topics include but are not limited to:


        <ul>
          <li>
            <b> Multimodal or Massive-Scale (Single Task Models): </b>
            Analyses that examine differences in evaluations of narrow
            (single modality, single task) and multipurpose AI and the
            limitations/challenges of existing metrics/benchmarks.
          </li>

          <li>
            <b> Multipurpose Models or Emergent Behavior (Models supporting Multiple Tasks): </b>
            Analyses that evaluate performance or behavior of multipurpose models
            (e.g., foundation models, neural platforms) that can support
            multiple tasks across single or multiple modalities.
            This includes zero-shot, few-shot, and finetuned tasks as well as emergent behavior.

          </li>

          <li>
            <b> Nontraditional Evaluation/Interpretability Methods: </b>
            Nontraditional methods, measures, or metrics targeting
            limitations of existing evaluation and interpretability of
            multimodality, multiple tasks (multipurpose), or emergent behavior.

          </li>
        </ul>


        <br>
        Submissions may incorporate the following:


        <ul>
          <li>
            Evaluations that encompass multiple principles of
            responsible AI/ML (e.g., robustness, accountability,
            fairness, transparency, interpretability) for multimodal,
            multipurpose, or massive-scale models

          </li>

          <li>
            Domain-driven evaluations for performance or interpretability
            needs of different use cases (e.g., commercial, academic)
            and users (e.g., researchers, domain scientists, practitioners,
            students)

          </li>
        </ul>

        The workshop will be organized around the complexity
        (multimodal, multipurpose, or massive-scale) of AI models
        under analysis or for which nontraditional methods of evaluation
        are developed to support, or traditional methods of
        evaluation/interpretability are extended.

        <br><br>
        </p>

        <h3> Special Themes for Position Papers “Multimodal, Multipurpose, or Massive-scale models and Beyond” </h3>

        <p>
          We are delighted to seek submissions for special themes that
          reflect progress and future directions in this area. These themes
          will include presentations and panel discussions around open questions,
          major obstacles, and integration of new techniques for NLP.
          These themes lend themselves to position papers, however, contributions
          with empirical evidence are encouraged as well.

        <ul>
          <li>
            Quantum Natural Language Processing: How does or will quantum natural language processing offer insight for evaluation and interpretability with NLP models? How does it align or not align with current evaluation and interpretability.
          </li>
          <li>
            Interpretability-Performance Tradeoffs: How should developers, end users, or evaluation hndle complementary or competing constraints when considering both performance and interpretability/explainability needs and requirements.
          </li>
        </ul>



        <h3> Topics </h3>

        <p>
          We encourage submissions from a wide range of topics which include but are not limited to:
        <ul>
          <li>
            Interpretability for large language models
          </li>
          <li>
            Multi-modal applications and models
          </li>
          <li>
            Efficiency in large model paradigm
          </li>
          <li>
            Multi faceted evaluation of large models
          </li>
          <li>
            Quantum Natural Language Processing
          </li>
          <li>
            Domain specific adaptations of large language models
          </li>

        </ul>

        </p>


        <h3> Submission Requirements </h3>
        <p>
          The workshop will accept ARR-reviewed papers as well as direct submissions to the SoftConf portal (<a href="https://www.softconf.com/coling2022/PIEM3SM/" target="_blank">https://www.softconf.com/coling2022/PIEM3SM/<a>).

              <ul>
                <li>
                  Any <a style="font-weight:bold; color:#5a4fcf;">ARR-reviewed paper</a>
                  that has all of its reviews and meta-reviews available by the workshop
                  submission deadline (<b>July 15, 2022</b>), can be committed to the workshop.
                  Submissions from ARR cannot be modified except that they can be associated with an author response.
                  <br>
                </li>

                <li>
                  Any <a style="font-weight:bold; color:#5a4fcf;">non-ARR paper</a>
                  can be directly submitted to the workshop’s submission portal by <b>July 15, 2022</b>.
                </li>
              </ul>

              <b>Format:</b> Submissions must follow COLING 2022 formatting: submissions of up to nine (9) pages maximum,
              excluding references, for long papers, and up to four (4) pages, excluding references, for short papers (including position papers submitted to address the special themes).
              For both long and short papers, the abstract should be no more than 200 words.

              <br>
        </p>


        <h3>Important Dates:</h3>

        <p>
          All deadlines are midnight UTC-12, anywhere on earth.
          <br>
        <ul>
          <li>
            <b>Papers Due (via Softconf): </b> July 15, 2022 (Friday)
          </li>
          <li>
            <b>Commitment of ARR Reviews by: </b> July 15, 2022 (Friday)
          </li>
          <li>
            <b>Notification of Acceptance:</b> August 30, 2022 (Tuesday)
          </li>
          <li>
            <b>Camera-ready papers due:</b> September 5, 2022 (Monday)
          </li>
          <li>
            <b>Conference date:</b> October 12-17, 2022
          </li>
        </ul>
        </p>
      </div>

    </div>

    <script>
      function openTab(evt, tabName) {
        var i, tabcontent, tablinks;
        tabcontent = document.getElementsByClassName("tabcontent");
        for (i = 0; i < tabcontent.length; i++) {
          tabcontent[i].style.display = "none";
        }
        tablinks = document.getElementsByClassName("tablinks");
        for (i = 0; i < tablinks.length; i++) {
          tablinks[i].className = tablinks[i].className.replace(" active", "");
        }
        document.getElementById(tabName).style.display = "block";
        evt.currentTarget.className += " active";
      }
    </script>

</body>

</html>
